{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFDS로 input pipeline 구축하기\n",
    "\n",
    "다음 colab을 따라한 것임\n",
    "* https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic mechanism\n",
    "\n",
    "* from_tensor_slice로 Dataset 생성 \n",
    "* for in 이나 next로 개별 data 접근하기\n",
    "* lambda 함수 적용 예시 (reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1,2,3])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for elem in dataset:\n",
    "    print(elem.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.data.ops.iterator_ops.OwnedIterator at 0x7f6252fde7b8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(dataset)\n",
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(next(it).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFDS.TypeSpec 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(10,), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n",
    "dataset1.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(100,), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random.uniform([4]), tf.random.uniform([4,100], maxval=100, dtype=tf.int32)))\n",
    "dataset2.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(10,), dtype=tf.float32, name=None),\n",
       " (TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(100,), dtype=tf.int32, name=None)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "dataset3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensorSpec(TensorShape([3, 4]), tf.int32)\n",
      "<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>\n"
     ]
    }
   ],
   "source": [
    "# with sparse tensor\n",
    "dataset4 = tf.data.Dataset.from_tensors(\n",
    "    tf.SparseTensor(indices=[[0,0], [1,2]], values=[1,2], dense_shape=[3,4]))\n",
    "print(dataset4.element_spec)\n",
    "print(dataset4.element_spec.value_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy array 로 dataset 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((28, 28), ()), types: (tf.float64, tf.uint8)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "images, labels = train\n",
    "images = images / 255\n",
    "\n",
    "print(type(images))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python generator로 Dataset 생성하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# generator \n",
    "def pyfunc_count(stop):\n",
    "    i = 0\n",
    "    while i<stop:\n",
    "        yield i\n",
    "        i +=1\n",
    "        \n",
    "for n in pyfunc_count(3):\n",
    "    print(n)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_counter = tf.data.Dataset.from_generator(\n",
    "    pyfunc_count, \n",
    "    args=[3], \n",
    "    output_types=tf.int32,\n",
    "    output_shapes=(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for count in ds_counter.take(10):   # single element generation\n",
    "    print(count.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 0 1]\n",
      "[2 0 1 2 0]\n",
      "[1 2 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "for count_batch in ds_counter.repeat().batch(5).take(3): # mini-batch generation\n",
    "    print(count_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가변 series를 generate 하고, padded batch 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : [-0.2034  1.9789  0.1841  0.8326]\n",
      "1 : [0.0763 0.0736 0.3123 0.1224]\n",
      "2 : [-1.2074 -0.4304]\n",
      "3 : [ 1.8337 -0.0877  0.395  -0.2962]\n",
      "4 : [ 0.8646  0.7961 -1.0588 -0.5157]\n",
      "5 : [-0.125  -1.2264 -0.381   0.1834]\n",
      "6 : [-0.0046  0.9751 -1.1306]\n"
     ]
    }
   ],
   "source": [
    "def gen_series():\n",
    "    i = 0\n",
    "    while True:\n",
    "        size = np.random.randint(0, 5)\n",
    "        yield i, np.random.normal(size=(size,))\n",
    "        i += 1\n",
    "        \n",
    "for i, series in gen_series():\n",
    "    print(i, \":\", str(series))\n",
    "    if i > 5:\n",
    "        break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: ((), (None,)), types: (tf.int32, tf.float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_series = tf.data.Dataset.from_generator(\n",
    "    gen_series,\n",
    "    output_types=(tf.int32, tf.float32),\n",
    "    output_shapes=((), (None, )))    # None으로 가변형 표현 \n",
    "ds_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.301 ,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    ,  0.    ],\n",
       "       [-0.4074,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    ,  0.    ],\n",
       "       [-1.3198,  1.3002,  0.8058,  0.5828]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_series_batch = ds_series.padded_batch(5)\n",
    "\n",
    "ids, seq_batch = next(iter(ds_series_batch))\n",
    "\n",
    "seq_batch.numpy()   # zero-padded 고정 길이형 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generator로부터 dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 images belonging to 5 classes.\n",
      "float32 (32, 256, 256, 3)\n",
      "float32 (32, 5)\n"
     ]
    }
   ],
   "source": [
    "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)\n",
    "\n",
    "images, labels = next(img_gen.flow_from_directory(flowers)) # tf.keras의 generator\n",
    "\n",
    "print(images.dtype, images.shape)\n",
    "print(labels.dtype, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(32, 256, 256, 3), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(32, 5), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_generator(\n",
    "    lambda: img_gen.flow_from_directory(flowers),\n",
    "    output_types = (tf.float32, tf.float32),\n",
    "    output_shapes = ([32,256,256,3], [32,5])\n",
    ")\n",
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 images belonging to 5 classes.\n",
      "<dtype: 'float32'> (32, 256, 256, 3)\n",
      "<dtype: 'float32'> (32, 5)\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(ds))   # tfds로부터 iteration\n",
    "\n",
    "print(images.dtype, images.shape)\n",
    "print(labels.dtype, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord 로부터 TFRecordDataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset(filenames=[fsns_test_file])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes_list {\n",
       "  value: \"Rue Perreyon\"\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_example = next(iter(dataset))\n",
    "parsed = tf.train.Example.FromString(raw_example.numpy())\n",
    "parsed.features.feature['image/text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text data로부터 TextLineDataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt\n",
      "819200/815980 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt\n",
      "811008/809730 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt\n",
      "811008/807992 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 파일들 가져오기\n",
    "directory_url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "file_names = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "file_paths = [\n",
    "    tf.keras.utils.get_file(file_name, directory_url + file_name)\n",
    "    for file_name in file_names\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TextLineDatasetV2 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.TextLineDataset(file_paths)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\"\n"
     ]
    }
   ],
   "source": [
    "line = next(iter(dataset))\n",
    "print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interleave를 사용해서 여러 개의 file로부터 온 line들을 섞기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\"\n",
      "b\"\\xef\\xbb\\xbfOf Peleus' son, Achilles, sing, O Muse,\"\n",
      "b'\\xef\\xbb\\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought'\n",
      "\n",
      "b'His wrath pernicious, who ten thousand woes'\n",
      "b'The vengeance, deep and deadly; whence to Greece'\n",
      "b'countless ills upon the Achaeans. Many a brave soul did it send'\n",
      "\n",
      "b\"Caused to Achaia's host, sent many a soul\"\n",
      "b'Unnumbered ills arose; which many a soul'\n",
      "b'hurrying down to Hades, and many a hero did it yield a prey to dogs and'\n"
     ]
    }
   ],
   "source": [
    "files_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3)\n",
    "\n",
    "for i, line in enumerate(lines_ds.take(9)):\n",
    "    if i % 3 == 0:\n",
    "        print()\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불필요한 line skip 및 line filter 조건 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone'\n",
      "b'0,male,22.0,1,0,7.25,Third,unknown,Southampton,n'\n",
      "b'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'\n",
      "b'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'\n",
      "b'1,female,35.0,1,0,53.1,First,C,Southampton,n'\n",
      "b'0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y'\n",
      "b'0,male,2.0,3,1,21.075,Third,unknown,Southampton,n'\n",
      "b'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'\n",
      "b'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n'\n",
      "b'1,female,4.0,1,1,16.7,Third,G,Southampton,n'\n"
     ]
    }
   ],
   "source": [
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "titanic_lines = tf.data.TextLineDataset(titanic_file)\n",
    "\n",
    "for line in titanic_lines.take(10):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'\n",
      "b'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'\n",
      "b'1,female,35.0,1,0,53.1,First,C,Southampton,n'\n",
      "b'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'\n",
      "b'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n'\n",
      "b'1,female,4.0,1,1,16.7,Third,G,Southampton,n'\n",
      "b'1,male,28.0,0,0,13.0,Second,unknown,Southampton,y'\n",
      "b'1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y'\n",
      "b'1,male,28.0,0,0,35.5,First,A,Southampton,y'\n",
      "b'1,female,38.0,1,5,31.3875,Third,unknown,Southampton,n'\n"
     ]
    }
   ],
   "source": [
    "def survived(line):\n",
    "    return tf.not_equal(tf.strings.substr(line, 0, 1), \"0\")  \n",
    "    \n",
    "survivors = titanic_lines.skip(1).filter(survived)\n",
    "\n",
    "for line in survivors.take(10):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV -> Pandas -> Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'survived'          : 0\n",
      "  'sex'               : b'male'\n",
      "  'age'               : 22.0\n",
      "  'n_siblings_spouses': 1\n",
      "  'parch'             : 0\n",
      "  'fare'              : 7.25\n",
      "  'class'             : b'Third'\n",
      "  'deck'              : b'unknown'\n",
      "  'embark_town'       : b'Southampton'\n",
      "  'alone'             : b'n'\n"
     ]
    }
   ],
   "source": [
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "df = pd.read_csv(titanic_file)\n",
    "\n",
    "titanic_ds = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "\n",
    "feature_batch = next(iter(titanic_ds))\n",
    "for key, value in feature_batch.items():\n",
    "    print(\"  {!r:20s}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV 로부터 한번에 Dataset 만들기 (from disk directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (OrderedDict([(fare, (4,)), (class, (4,))]), (4,)), types: (OrderedDict([(fare, tf.float32), (class, tf.string)]), tf.int32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
    "    titanic_file, batch_size=4,\n",
    "    select_columns=['class', 'fare', 'survived'],\n",
    "    label_name=\"survived\"\n",
    ")\n",
    "titanic_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'survived': [1 0 1 0]\n",
      "  'fare'              : [ 26.2875   7.0542 146.5208  26.25  ]\n",
      "  'class'             : [b'First' b'Third' b'First' b'Second']\n"
     ]
    }
   ],
   "source": [
    "feature_batch, label_batch = next(iter(titanic_batches))\n",
    "print(\"'survived': {}\".format(label_batch))\n",
    "for key, value in feature_batch.items():\n",
    "    print(\"  {!r:20s}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV 로부터 한번에 CsvDataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CsvDatasetV2 shapes: ((), (), (), (), (), (), (), (), (), ()), types: (tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_types  = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string] \n",
    "dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types, header=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, b'male', 22.0, 1, 0, 7.25, b'Third', b'unknown', b'Southampton', b'n']\n"
     ]
    }
   ],
   "source": [
    "line = next(iter(dataset))\n",
    "print( [item.numpy() for item in line] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 복수 파일로부터 dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daisy\n",
      "tulips\n",
      "roses\n",
      "dandelion\n",
      "sunflowers\n",
      "LICENSE.txt\n"
     ]
    }
   ],
   "source": [
    "flowers_root = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)\n",
    "flowers_root = pathlib.Path(flowers_root)\n",
    "\n",
    "for item in flowers_root.glob(\"*\"):\n",
    "    print(item.name)   # subdirectory로 개별 클래스 인스턴스 파일들을 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/home/hoondori/.keras/datasets/flower_photos/tulips/7094415739_6b29e5215c_m.jpg'\n",
      "b'/home/hoondori/.keras/datasets/flower_photos/roses/6687138903_ff6ae12758_n.jpg'\n",
      "b'/home/hoondori/.keras/datasets/flower_photos/daisy/18023717391_e2c9089e10.jpg'\n",
      "b'/home/hoondori/.keras/datasets/flower_photos/sunflowers/4933823922_911ac40b0d.jpg'\n",
      "b'/home/hoondori/.keras/datasets/flower_photos/dandelion/4226758402_a1b75ce3ac_n.jpg'\n"
     ]
    }
   ],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n",
    "\n",
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x03\\x02\\x02\\\n",
      "\n",
      "b'dandelion'\n"
     ]
    }
   ],
   "source": [
    "# path에서 label을 추출해서 label을 가진 ds 만들기\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = tf.strings.split(file_path, os.sep)[-2]\n",
    "    return tf.io.read_file(file_path), label\n",
    "\n",
    "labeled_ds = list_ds.map(process_path)\n",
    "\n",
    "image_raw, label_text = next(iter(labeled_ds))\n",
    "print(repr(image_raw.numpy())[:100])\n",
    "print()\n",
    "print(label_text.numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchDataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[1]\n",
      "[2 2]\n",
      "[3 3 3]\n",
      "[4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.map(lambda x: tf.fill([tf.cast(x,tf.int32)],x))\n",
    "\n",
    "for elem in dataset.take(5):\n",
    "    print(elem.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 0]\n",
      " [2 2]]\n",
      "[[3 3 3 0 0]\n",
      " [4 4 4 4 0]\n",
      " [5 5 5 5 5]]\n",
      "[[6 6 6 6 6 6 0 0]\n",
      " [7 7 7 7 7 7 7 0]\n",
      " [8 8 8 8 8 8 8 8]]\n",
      "[[ 9  9  9  9  9  9  9  9  9  0  0]\n",
      " [10 10 10 10 10 10 10 10 10 10  0]\n",
      " [11 11 11 11 11 11 11 11 11 11 11]]\n",
      "[[12 12 12 12 12 12 12 12 12 12 12 12  0  0]\n",
      " [13 13 13 13 13 13 13 13 13 13 13 13 13  0]\n",
      " [14 14 14 14 14 14 14 14 14 14 14 14 14 14]]\n"
     ]
    }
   ],
   "source": [
    "# padded batch 만들기: Mini-Batch 안의 가장 긴 시퀀스대로 패치한다.\n",
    "\n",
    "pad_dataset = dataset.padded_batch(batch_size=3, padded_shapes=(None,))\n",
    "\n",
    "for elem in pad_dataset.take(5):\n",
    "    print(elem.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
